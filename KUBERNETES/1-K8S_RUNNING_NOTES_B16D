
1. What Docker & Docker Compose Do Well ???

Docker helps you:

      Package your app and dependencies into portable containers.      
      Run the same container anywhere (dev, test, prod).      
      Isolate processes and environments.
      
Docker Compose adds:

    Multi-container setup — e.g., running a web app + DB + Redis + xyz ..etc  together.    
    Simple configuration — define everything in docker-compose.yml.    
    Great for local development and small deployments.
    
So far, so good , we can 

    Package apps consistently (Dockerfile) 
    Define multiple containers easily (docker-compose.yml)
    Run them locally with one command
    Even auto-restart containers with restart: always



CHALLENGES WITH DOCKER &  DOCKER COMPOSE 
========================

1 Scaling and High Availability

     Docker Compose doesn’t automatically scale containers across multiple machines.     
     If one container/node fails, you need to manually restart or handle failover.     
     No built-in load balancing or auto-recovery.

2 Multi-host / Cluster Management
      Docker Compose works mostly on a single host (machine)
      Real production systems need to run across multiple servers or clouds — Compose doesn’t handle that.

3. Self-healing & Monitoring
      If a container crashes, Compose won’t automatically reschedule or replace it.   
      You need to handle health checks and restarts manually.

4. Rolling Updates / Zero-downtime Deployments

      Docker Compose lacks advanced deployment strategies (rolling updates, blue-green, canary, etc.).

5. Networking & Service Discovery

      Compose’s network works fine locally, but in a distributed environment you need dynamic service discovery, DNS, and load balancing — which Compose doesn’t provide natively.
	  
6. Resource Management

     You can set limits, but no cluster-level scheduling based on CPU/RAM usage	 
	 
7.  Monitoring and logging need extra tools	 
	 


- DOCKER CAN NOT ORCHESTRATE

==============================================
DOCKER 

CONT.ORC CONCEPTS ---> DOCKER SWARM / K8S 


-------------------------------------------------------
WHY KUBERNETES (K8S):
=========================
- Production-grade scaling
- High Availability 
- Self-healing: 
- Automated rollouts , rollback & zero downtime
- Automates container orchestration & Automated Scheduling (hpa) 
- Scales containers up or down easily
- Handles networking across clusters
- Built-in load balancing
- Simplifies deployment and updates



========================================================================
What is Kubernetes?
•	Kubernetes is an orchestration engine and open-source platform for managing containerized applications.
•	Born in Google ,written in Go/Golang. Donated to CNCF(Cloud native computing foundation) in 2014.
•	Kubernetes v1.0 was released on July 21, 2015.


FEATURES:

•	Automated Scheduling: 
Kubernetes provides advanced scheduler to launch container on cluster nodes based on their resource
requirements and other constraints, while not sacrificing availability.

•	Self Healing Capabilities:
 Kubernetes allows to replaces and reschedules containers when nodes die. It also kills containers that don’t
respond to user-defined health check and doesn’t advertise them to clients until they are ready to serve.

•	Automated rollouts & rollback: 
Kubernetes rolls out changes to the application or its configuration while monitoring application health
to ensure it doesn’t kill all your instances at the same time. If something goes wrong, with Kubernetes you can rollback the change.

•	Horizontal Scaling & Load Balancing:
 Kubernetes can scale up and scale down the application as per the requirements with a simple
command, using a UI, or automatically based on CPU usage.

=================================================================================


SOFTWARES:  eksctl , kubectl , aws cli , visual studo code (ide) 

MINIKUBE , KIND 


aws  - EKS 
azure - AKS
gcp - GKE

1. chacolety 
--------------------------------------
2. eksctl -- CLUSTER CREATION 
 eksctl version
0.215.0

--------------------------------------

3. kubectl -- CLI TO INTERACT WITH CLUSTER ( API SERVER) 
 kubectl version --client
Client Version: v1.34.1-eks-113cf36
Kustomize Version: v5.7.1

--------------------------------------
4. AWS CLI 
 aws --version
aws-cli/2.23.9 Python/3.12.6 Windows/11 exe/AMD64

ACCESS KEY 
SECRET ACCESS KEY 

-----------------------------------------

5. visual studio code  (IDE ) 
   - For syntax auto suggestions / syntax auto completion 
   - easy to manage the code / script 

============================

Installing on linux machine :: 

kubectl & eksctl installation on aws linux machine 

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"


curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"


sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl


============================================================


WHY K8S 

High availability , Zero downtime , 

K8S ARCHITECTURE:
================

Kubernetes Architecture is based on a master-worker model, where the Master manages and controls the cluster, and the Workers run the containerized applications. Let's break down the architecture and its core components.

KUBERNETES ARCHITECTURE :

Master Node:    The control plane responsible for managing the Kubernetes cluster.
Worker Nodes:   The nodes that run the containerized applications (pods).
Cluster:        A set of worker nodes managed by the master node.


---------------------------------------

1. MASTER NODE (CONTROL PLANE): 
============================
The Master Node is the brain of the Kubernetes cluster. 
It manages the cluster, maintains the desired state, schedules workloads, and monitors the overall health of the cluster.
It consists of several critical components:

     KUBE-APISERVER:     
     The API server is the central point of contact for all Kubernetes components. It handles requests from clients, whether that’s the kubectl CLI, the Kubernetes UI, or other components in the system.     
     The API server exposes the REST API, which is used by external clients to interact with Kubernetes. It ensures the system’s state is updated.
	 
    -  The entry point /Gateway to the Kubernetes cluster.
    -  All commands (like kubectl) talk to the API server.
    -  Think of it as the front desk of K8s.
	   
	 
	 
     ETCD:     
 
	- A key-value database that stores all cluster data — pods, deployments, secrets, etc.
	- It is highly available and persistent - Kubernetes can recover the cluster's state even after a failure.
	- etcd ensures consistency across the entire cluster.
    - It’s like Kubernetes’ memory.
     
	 
     KUBE-SCHEDULER:     
     
	- Watches for new pods that need to be scheduled.
    - Decides which node a pod should run on (based on resources, affinity rules, etc)  
    - ensures that workloads are distributed across nodes in an efficient and optimal manner.
     
     
	 KUBE-CONTROLLER-MANAGER:     
	 
	- Runs various controllers (like replica controller, node controller, etc.) 
    - Ensure that the cluster's desired state is maintained. (desired state = actual state) 
    - Ex: if you said you want 3 replicas but only 2 are running, it creates 1 more. 

     
     CLOUD-CONTROLLER-MANAGER (OPTIONAL):     
	 
    - This component is used when running Kubernetes in a cloud environment. 
	- Interacts with the underlying cloud provider (e.g., AWS, AZURE , Google Cloud) to manage resources like    load balancers, volumes, and instances.
	- It helps in maintaining cloud-specific services in coordination with Kubernetes.
     
	 
2. WORKER NODE COMPONENTS:
===========================

     A Worker Node (also called a Minion) runs the applications and workloads in the form of Pods. Each worker node contains the following components:
     
     KUBELET:     
	 
	- The agent on each worker node that talks to the API Server.
	-  Makes sure containers are running as instructed.
	- takes action to rectify any issues (e.g., restarting a failed container).
	
     
	 
     KUBE-PROXY:     
	 
	- Handles networking — ensures pods can talk to each other and the internet.
	- Manages load balancing and routing traffic to the right pod.     
	
     
     CONTAINER RUNTIME (DOCKER/ cri-o / containerd)  

    - Runs the containers inside a pod (Docker, containerd, CRI-O, etc).
    - It pulls container images, starts containers, and manages the lifecycle of containers within the pods.
	 
======================================     
     PODS:     
	 
	- Pod : The Smallest Deployable Unit in K8S
    - Pod is the basic building block in Kubernetes.
	- It wraps one or more containers that share the same network and storage.
	- Usually, one container per pod (but sometimes more if they’re tightly coupled).
	 

So who actually makes the pod run ? 
   
   
   API Server - Accepts pod creation requests, validates and stores them in etcd 
   Scheduler - Watches for unscheduled pods, selects a node - Decides which node should host the pod
   API Server - Updates Pod with node assignment     
   Kubelet	-  on the chosen node , talks to Container Runtime to  create containers, 
   
   
 So kubelet Actually runs and monitors the pod on its node, kubelet reports status back to API SERVER 

=============================================================================================


Where can you set up a Kubernetes cluster?

1. Self-managed cluster - You install and manage everything yourself.
    Treditional approach - Self managed , barematel , customer managed k8s cluster 
    Kubeadm → install K8s manually on servers (on-prem or in the cloud).
	
	
  Kubernetes Control Plane - Self Managed  
        Need to make Control Plane Highly Available
        Maintain multiple EC2 in multiple AZ
        Scale Control Plane if needed
        Keep etcd up and running
        Overhead of managing EC2s
        Security Patching
        Replace failed EC2s/vms/physical servers
        Orchestration for Kubernetes Version Upgrade

2. Managed Kubernetes services (cloud)
    
	 k8s cluster as a service , Cloud providers give you a ready-to-use Kubernetes control plane.
     You just focus on your apps, not managing Kubernetes itself.

         AWS	   EKS (Elastic Kubernetes Service)
         GCP      GKE (Google Kubernetes Engine)
         Azure	   AKS (Azure Kubernetes Service)



   
   CLOUD MANAGED KUBERNETES ( EKS / AKS / GKE ) 

       Kubernetes Control Plane - AWS Managed - eks
       
       ● AWS Manages Kubernetes Control Plane 
       ● Creates and manages the Control Plane for you (API server, etcd, scheduler, etc.)
       ● Handles upgrades, scaling, and security patches automatically.
       ● AWS maintains High Availability - Multiple EC2s in Multiple AZs
       ● AWS Detects and Replaces Unhealthy Control Plane Instances
       ● AWS Scales Control Plane
       ● Provides Automated Version Upgrade and Patching


   in EKS you only need to manage Data Plane 

   ● Create Worker Nodes (EC2 instances or Fargate tasks)   - we will use managed node groups 
   ● Deploy our  Pods and apps.
   ● Cluster configuration
   ● Security policies
   ● Monitoring/logging
   
   
   

EKS K8S CLUSTER SETUP 


====================================================

Ways To Spin Up EKS Cluster:

           
           eksctl CLI
		   
		   
          
What is eksctl - CloudFormation stack
  

     ● CLI tool for creating clusters on EKS
     ● Easier than console
     ● Abstracts lots of stuff - VPC, Subnet, Sec. Group , nodes , docker etc.
     using CloudFormation stack 

=======================	   
aws configure

Accesskey 
Secret Access key 
---
AKIASB5MXXXXXXXXXXXXXXXXXXXXX
DSR3tgUbjV1KgwglXXXXXXXXXXXXXXX

1. awscli & configufre 
aws sts get-caller-identity

once configure is done , our terminal will have admin accss to our aws account 


=======================================================
eksctl create cluster  

--> it creates  two m5.large large nodes by default for data plane . Insted go for t3.micro

  
     Control Plane - AWS WILL TAKE CARE !   
     Data Plane  - 2 m5.large worker nodes 


eksctl create cluster --name <name> --version <> --nodegroup-name <> --node-type t3.micro --nodes 4 --managed 



eksctl create cluster --name b16dcluster  --nodegroup-name b16ng --node-type t3.micro --nodes 4 --managed 

=======================================================


eksctl get cluster 
eksctl delete cluster <clustername> 
=============================================================


2024-11-07 19:03:13 [ℹ]  waiting for CloudFormation stack "eksctl-b15cluster-cluster"
:
:
2024-11-07 19:34:33 [ℹ]  waiting for CloudFormation stack "eksctl-b15cluster-nodegroup-b15ng"
:
:
2024-11-07 19:37:57 [ℹ]  nodegroup "b15ng" has 4 node(s)
2024-11-07 19:37:57 [ℹ]  node "ip-192-168-1-245.ec2.internal" is ready
2024-11-07 19:37:57 [ℹ]  node "ip-192-168-30-5.ec2.internal" is ready
2024-11-07 19:37:57 [ℹ]  node "ip-192-168-48-160.ec2.internal" is ready
2024-11-07 19:37:57 [ℹ]  node "ip-192-168-52-207.ec2.internal" is ready
2024-11-07 19:37:57 [✔]  created 1 managed nodegroup(s) in cluster "b15cluster"
2024-11-07 19:38:01 [ℹ]  kubectl command should work with "C:\\Users\\Dell\\.kube\\config", try 'kubectl get nodes'
2024-11-07 19:38:01 [✔]  EKS cluster "b15cluster" in "us-east-1" region is ready


-------------------------------------------------------------
CONTROL PLANE (MASTER NODES) :

Kubernetes master nodes are distributed across several AWS availability zones (AZ), and traffic is managed by Elastic Load Balancer (ELB).

EKS is a managed service, so you have no direct access to the master nodes. And you don't need the access.
==================================

pod - Containers are encapsulated into a k8s object called pod ,in simple words pod is a abstraction on containers 

Pod can have multiple continers but in general not of same container type (mostly helper container) 	

--------------------------------------------
Creating a POD from Commandline : 


docker run --name cot1 image
kubectl run testpod --image=tomcat 


history:

 eksctl create cluster --name testcluster  --nodegroup-name testng --node-type t3.micro --nodes 5 --managed
 eksctl get cluster
 kubectl get nodes
 kubectl run testpod --image tomcat
 kubectl get pods
 kubectl describe pod testpod
 kubectl run pd2 --image tomicaaaat
 kubectl get pods
 kubectl describe pod pd2
 kubectl get pods
 kubectl run pod3 --image amazonlinux
 kubectl get pods
 kubectl get pods
 kubectl describe pod pod3
 eksctl get cluster
 eksctl delete cluster testcluster


DISCUSSED : 

IMAGE PULL BACKOFF 
CRASHLOOP BACK OFF ERROR 

================================================================

DATE: 13 OCT 25 

kubectl cluster-info
kubectl cluster-info --dump
=================================================================

kubectl run test1k8s --image=tomcat

kubectl get pods
kubectl get pods -o wide 
kubectl delete pod testk8s


==========================================================================================

yaml - one of the data structure format to represent data 
       yaml file is used to represent data , in our case k8s object configuration data

for ex : 

server details :

name = insta
created = 13-10-2025 
status = active 
owner = Madhu

name = whatsapp
owner = kiran
created = 13-10-2025 
status = active 

====================================

servers:


  - name: whatsapp
    owner: kiran
    created: 2025-10-13
    status: active

  - name: insta
    owner: Madhu
	status: active
    created: 2025-10-13
   
	


--- show above server data as xml , yaml and json format 

-----------------------------------
key value pair - 

name: server1

-------------------
Array/ List :

Fruits:
- grape   
- banana
- Orange: 
    Carbs: 5g   
	Fat: 0.1 g
	Calories: 50


  
- apple
    Fat: 0.1 g
    Carbs: 5g
    Calories: 50
    
    
-------------------

Dictionary - Set of properties , grouped together

Orange: 
   Calories: 50
   Fat: 0.1 g
   Carbs: 5 g
   
   
make sure we are giving spaces properly 
 - equal no of spaces   
----------------------------------------






INSTALL VSCODE --> INSTALL YAML EXTENSION --> GO TO SCHEMA: SETTING.json

=================


    "yaml.schemas": {    

        "kubernetes": "*.yaml" 
        
    },

==========================================================

cat instapod1.yaml


instapod1.yaml
---------------------------

apiVersion: v1
kind: Pod
metadata:
  name: fbpod
  labels:
    app: facebook
    env: lab 
spec:
  containers:
    - name: fbcont
      image: tomcat

=======================================

kubectl get pod
kubectl describe pod fbpod
kubectl get po
kubectl delete pod fbpod
kubectl get po

=======================================================================================
REPLICA SET: 




spec : we will create a template section under spec to provide a pod template to be used by Replication contoller/ Relica set to create replicas 


Replication Controller vs Replica set :

   Replica set requires a selector defination 
   i,e 
   seletor is not mandate for RC ,
   but for Replica set it is mandate , a user input is required for this property like match labels 


FOR REEPLICA SET manifest files ---> -
check for  manifest files in zip folder

=======================================================================================
DATE: 14 OCT 24 

--------
kubectl run pod nginx --image nginx:latest --dry-run -o yaml 

--------
kubectl create deployment instadeploy --image=devopshubg333/batch15d:mcappimag --replicas=3 --dry-run -o yaml

--------
kubectl explain pods ---> gives explaination of pods manifest files 

--------
kubectl explain pods.metadata

--------
kubectl cluster-info
kubectl cluster-info dump ---> total cluster info

--------

kubectl api-resources --->resorces we can create like podscontainers,deployments...etc

kubectl api-resources | grep pod 
--------
kubectl cordon instance ip ---> unschedule the nodes ,no new pod will be schedule
Marks a node as unschedulable—new pods will not be scheduled on it, but existing pods continue to run.. 
when You want to stop placing new workloads on the node but don't want to disrupt what's already running.
--------

kubectl drain ip-172-20-103-135.ec2.internal --->  evicts all pods to another node and schedule will be disable
--------
kubectl uncordon ip-172-20-38-30.ec2.internal


kubectl get all 
==========================		
kubectl get replicaset
NAME     DESIRED   CURRENT   READY   AGE
tomcat   5         5         5       116s
-----------

So Why do we need labels and Selectoers ?

Labels can be used as filter to replica set , so replica set will monitor and ensure desired no of pods are running 

==========================================================
scale replicas and ways to scale :

1. kubectl scale replicaset insta-rs --replicas=3
1. Change manifest file -->- then kubectl apply -f insta-rs.yaml
3. kubectl scale --replicas=6 -f insta-rs.yaml
4. kubectl edit replicaset <replicasetname> --> opens a temporoay file in memory


vim insta-rs.yaml

---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: insta-rs 
  labels:
    app: instagram 
    env: production 

spec:
  selector:
    matchLabels:
      app: instagram 

  replicas: 2
  template:
    metadata:
      name: insta-pod
      labels:
        app: instagram 

    spec:
      containers:
        - name: insta-pod-cont
          image: devopshubg333/batch16d:fb_httpd
          ports:
            - containerPort: 80

====================================================
HISTORY 

 ll 13oct25-pods/
  526  kubectl apply -f 13oct25-pods/pod1.yaml
  527  kubectl get po
  528  kubectl describe po insta-pod
  529  kubectl run pod nginxpod --image nginx:1.14.2 --dry-run -o yaml
  530  kubectl api-resources
  531  kubectl api-resources
  532  kubectl api-resources | grep pod
  533  kubectl cluster-info
  534  kubectl cluster-info dump
  535  kubectl explain pods
  536  kubectl explain rs
  537  kubectl explain pods.spec
  538  kubectl explain pods.metadata
  539  kubectl get pods
  540  kubectl get nodes
  541  kubectl get nodes -o wide
  542  kubectl get pods -o wide
  543  kubectl get nodes
  544  kubectl cordon ip-192-168-21-164.ec2.internal
  545  kubectl get nodes
  546  kubectl get po
  547  kubectl get po -o wide
  548  kubectl get nodes
  549  kubectl uncordon ip-192-168-21-164.ec2.internal
  550  kubectl get nodes
  551  kubectl get po -o wide
  552  kubectl drain ip-192-168-21-164.ec2.internal
  553  kubectl drain ip-192-168-21-164.ec2.internal--ignore-daemonsets
  554  kubectl drain ip-192-168-21-164.ec2.internal --ignore-daemonsets
  555  kubectl drain ip-192-168-21-164.ec2.internal --ignore-daemonsets --force
  556  kubectl get po -o wide
  557  kubectl get po -o wide
  558  kubectl get nodes
  559  kubectl uncordon ip-192-168-21-164.ec2.internal
  560  kubectl delete po pod1
  561  kubectl explain rs
  562  kubectl api-resources | grep replicaset
  563  ll
  564  cd 14oct25/
  565  ll
  566  cat insta-rs.yaml
  567  kubectl get all
  568  kubectl get po -o wide
  569  kubectl delete po insta-rs-4rdcz
  570  kubectl delete po
  571  kubectl get po
  572  kubectl get po -o wide
  573  kubectl delete po insta-rs-w6mv9
  574  kubectl get po -o wide
  575  kubectl get rs
  576  kubectl describe rs insta-rs
  577  kubectl get all
  578  ll
  579  kubectl apply -f insta-rs.yaml
  580  kubectl get all
  581  kubectl apply -f insta-rs.yaml
  582  kubectl get all
  583  kubectl scale rs insta-rs --replicas 9
  584  kubectl get all
  585  kubectl get all
  586  kubectl get all
  587  kubectl describe rs insta-rs
  588  kubectl get po
  589  kubectl describe po insta-rs-rjscp
  590  kubectl scale rs insta-rs --replicas 4
  591  kubectl edit rs insta-rs
  592  kubectl get po
  593  kubectl get po -o wide
  594  kubectl get po -o wide
  595  kubectl get nodes
  596  kubectl drain ip-192-168-28-138.ec2.internal
  597  kubectl drain ip-192-168-28-138.ec2.internal --ignore-daemonsets
  598  kubectl get po -o wide
  599  kubectl get nodes
  600  kubectl get po -o wide
  601  kubectl uncordon ip-192-168-28-138.ec2.internal
  602  kubectl get po -o wide
  603  history



=====================================================================



DATE: 15 OCT 25

=====================================================================

To create a deployment using imperative command, use kubectl create:

kubectl create deployment nginx --image=nginx

kubectl create deployment instadeploy --image=<image> --replicas=3 --dry-run -o yaml
--------------------------------------------------------


cat 1-deploy_AC_V1.yaml


---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fb-deploy 
  labels:
    app: facebook
    env: lab 

spec:
  selector:
    matchLabels:
      app: facebook 
  replicas: 3  
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    
  template:
    metadata:
      name: fb-pod 
      labels:
        app: facebook
        env: lab 
        version: v1_AC
    spec:
      containers:
        - name: fb-pod-cont
          image: devopshubg333/batch16d:fb_httpd
      
====================
kubectl apply -f 02-DEPLOYMNET-PRACTICE/1-deploy_AC_V1.yaml

==================================================================

cat 2-deploy_VC_v2.yaml
---

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fb-deploy 
  labels:
    app: facebook
    env: lab 

spec:
  selector:
    matchLabels:
      app: facebook 
  replicas: 3  
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  
  template:
    metadata:
      name: fb-pod 
      labels:
        app: facebook
        env: lab 
        version: v2_VC
    spec:
      containers:
        - name: fb-pod-cont
          image: devopshubg333/batch16d:pythonimg
      
==============
kubectl apply -f 02-DEPLOYMNET-PRACTICE/1-deploy_AC_V1.yaml
kubectl apply -f 02-DEPLOYMNET-PRACTICE/2-deploy_VC_v2.yaml

 kubectl get deployment

------------

  519  cd 15oct25-deploy/
  520  ll
  521  cat 1-fbapp_ac.yaml
  522  kubecl apply -f 1-fbapp_ac.yaml
  523  kubectl apply -f 1-fbapp_ac.yaml
  524  kubectl get all
  525  kubectl describe deploy fb-deploy
  526  kubectl describe deploy fb-deploy
  527  ll
  528  kubectl apply -f 2-fbapp_vc.yaml
  529  kubectl get all
  530  kubectl describe deploy fb-deploy
  531  kubectl describe deploy fb-deploy
  532  kubectl rollout pause deploy fb-deploy
  533  kubectl rollout resume deploy fb-deploy
  534  kubectl get po
  535  kubectl get po  --watch
  536  kubectl get all
  537  kubectl describe deploy fb-deploy




kubectl create deployment instadeploy --image=devopshubg333/batch15d:mcappimag --replicas=3 --dry-run -o yaml

=============================================================================
kubectl get po --watch 
kubectl rollout history deploy fbdeploy

kubectl get rs 
==========================================================
kubectl rollout pause deploy fbdeploy
 deployment.apps/fbdeploy paused

====================
kubectl rollout resume deploy fbdeploy
 deployment.apps/fbdeploy resumed

===================
kubectl rollout undo deploy fbdeploy
deployment.apps/fbdeploy rolled back
==========================================

kubectl rollout history deploy fbdeploy


=====================================================

=====================================================================


DATE: 16 OCT 25

docker pull sravani2310/mcapp:latest


SERVICES:


 cat 1-fbapp_ac.yaml
 
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fb-deploy 
  labels:
    app: facebook
    env: lab 

spec:
  selector:
    matchLabels:
      app: facebook 
  replicas: 3  
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    
  template:
    metadata:
      name: fb-pod 
      labels:
        app: facebook
        env: lab
        tier: frontend 
        version: v1_AC
    spec:
      containers:
        - name: fb-pod-cont
          image: sravani2310/mcapp:latest
          ports:
            - containerPort: 8080
		
		
===================================================	============
cat lb-svc.yaml 
---

---
apiVersion: v1
kind: Service
metadata:
   name: fb-lb-svc
   labels:
     app: facebook 

spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 8080 
      protocol: TCP

  selector:
        app: facebook
        env: lab
      
      
		
===========================================================================
		
		
cat cluster-ip.yaml


---
apiVersion: v1
kind: Service
metadata:
   name: fb-cip-svc
   labels:
     app: facebook 

spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8080 
      protocol: TCP

  selector:
    app: facebook


===================================================================

17-OCT-25



cat nodeport.yaml

---
apiVersion: v1
kind: Service
metadata:
  name: fb-np-svc
  labels:
    app: facebook 
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 30333

  selector:
    app: facebook

==============================================================================

In Kubernetes, Service types are abstractions that define how to expose Pods. The common types are:

      ClusterIP (default)
      
      NodePort
      
      LoadBalancer

-------------------------------------------

flow :

                   Client (Internet)
                      ↓
                   External Load Balancer
                      ↓
                   NodePort (on a Kubernetes Node)
                      ↓
                   ClusterIP (Kubernetes internal)
                      ↓
                   Pod (endpoint)


----------------------------------------------

DATE:


----------------------------------------------

MINI PROJECT :

WITH PODS 

WITH DEPLOYMENT 


-----------------------------------------------------------------------------




NameSpaces :

Namespaces are Kubernetes objects which partition a single Kubernetes cluster into multiple virtual clusters

which means that using the combination of an object name and a Namespace, each object gets an unique identity across the cluster.
------------

A namespace in Kubernetes is a way to divide your cluster into smaller, isolated spaces for better organization and management.
------------

When you have a big Kubernetes cluster that runs many applications, you don’t want everything mixed together.
So, you create namespaces to separate and organize things.

Example:

Imagine you have one big Kubernetes cluster for your company.

You can make:

     dev namespace → for developers to test things     
     staging namespace → for testing before release     
     prod namespace → for real users
     
Each namespace can have its own:

       Pods (containers)       
       Services       
       Configurations       
       Resource limits (CPU, memory, etc.)

And Kubernetes makes sure that things in one namespace don’t clash with another — for example, you can have a web service in both dev and prod, and they won’t conflict.

--------------------------------------------------------

kubectl get pod -o wide
kubectl get pod -o wide --all-namespaces

=================================

kubectl get ns

kubectl create ns kirannamespace

kubectl apply  -f pod.yaml -n kirannamespace

kubectl get pods --all-namespaces

=========================================

Resource requests & limits :

Resource Requests
A request is the amount of CPU and memory that Kubernetes will allocate to a container. When you specify a request, Kubernetes ensures that your container will always have at least that much resource available when it runs

------------
LIMITS : 
The maximum amount of CPU and memory the container can use. 
If the container exceeds these, it may be throttled (CPU) or killed (Memory).



with out resource requests & limits :

---
apiVersion: v1
kind: Pod
metadata:
  name: insta-pod2 
  labels:
    app: instagram 
    env: production
spec:
  containers:
    - name: insta-cont 
      image: devopshubg333/batch16d:fb_httpd 

    - name: insta-side-car-cont
      image: devopshubg333/batch16d:fb_httpd 
	  
-------
     Kubernetes has no idea how much CPU/memory this needs.
     It might schedule it anywhere.     
     If the node gets busy, this pod could slow down or get evicted.     
     It can also use unlimited CPU/memory, possibly hurting other apps.	  
	  

========================================================


apiVersion: v1
kind: Pod
metadata:
  name: with-resources
spec:
  containers:
    - name: web
      image: nginx:latest
      resources:
        requests:
          cpu: "200m"     # needs at least 0.2 of a CPU core
          memory: "256Mi" # needs at least 256 MB of RAM
        limits:
          cpu: "500m"     # can use up to 0.5 CPU core
          memory: "512Mi" # can use up to 512 MB of RAM
		  
		  
---------------------------------------------------		  
   Kubernetes knows this pod needs a small amount of CPU/memory to start.   
   It will schedule it on a node that has enough room.   
   If the pod tries to use more than the limit   
       CPU: it just gets throttled (slowed down).       
       Memory: it gets killed (OOMKilled) to protect the node.		  
       

==========================================================================
how to log into the pod ? 

kubect exec -it <podname> -- /bin/bash
kubect exec -it <podname> -- env

----------------------

ENV vs CONFIG MAP vs  SECRET 

Environment Variables (ENV):

        environment variables are key–value pairs injected into the container’s runtime environment.
        App configs like port numbers, feature flags, etc.
        Env variables are Typically non-sensitive.
        Not encrypted.
        Easily visible in kubectl describe pod.

You can define them:

       Directly in a Pod spec
       Or indirectly using a ConfigMap or Secret
=====================

ENV:

--- 
apiVersion: v1
kind: Pod
metadata:
  name: envex-pod 

spec: 
  containers:
    - name: insta-pod-cont
      image: devopshubg333/batch16d:fb_httpd
      env:
        - name: Course
          value: AWS DevOps 

        - name: Trainer 
          value: Madhukiran Gorekar 

        - name: Institute
          value: MindCircuit 
    

=====================

ConfigMap

   ConfigMaps are Kubernetes API objects that store configuration data as key–value pairs (strings).
    They’re meant for non-sensitive settings.

Ways to use a ConfigMap:
      As environment variables
      As command-line arguments
      As files mounted into a container


configmap.yaml:


---
apiVersion: v1
kind: ConfigMap
metadata:
  name: mc-config

data:
  Course: AWS DevOps 
  Trainer: Madhukiran Gorekar
  Institute: MindCircuit


-------------------------------------------------

--- 
apiVersion: v1
kind: Pod
metadata:
  name: configmap-ex-pod 

spec: 
  containers:
    - name: insta-pod-cont
      image: devopshubg333/batch16d:fb_httpd

      envFrom:
        - configMapRef:
            name: mc-config

===========================================

Secret:

 Secrets are like ConfigMaps, but intended for sensitive data such as credentials or tokens.
Kubernetes stores them base64-encoded, not encrypted by default (though you can enable encryption at rest).

A Secret is used to store sensitive data, such as passwords, OAuth tokens, SSH keys, and API keys, in a way that is more secure than using a plain ConfigMap. 

Secrets are encoded (base64), which helps to obscure their content but is not a fully secure solution; they are more secure than environment variables or plain text files


encode:


 echo -n "AWS DevOps" | base64
QVdTIERldk9wcw==

echo -n "Madhukiran Gorekar" | base64
TWFkaHVraXJhbiBHb3Jla2Fy

echo -n "MindCircuit" | base64      -----> TWluZENpcmN1aXQ=




decode:
echo -n "QXdzIERldk9wcw==" | base64 --decode
echo -n "TWFkaHVraXJuYSBnb3Jla2Fy" | base64 -d
==================================

---
apiVersion: v1
kind: Secret
metadata:
  name: mc-secret
type: Opaque
data:
  Course: QVdTIERldk9wcw==
  Trainer: TWFkaHVraXJhbiBHb3Jla2Fy
  Institute: TWluZENpcmN1aXQ=


============================

--- 
apiVersion: v1
kind: Pod
metadata:
  name: secret-ex-pod 

spec: 
  containers:
    - name: insta-pod-cont
      image: devopshubg333/batch16d:fb_httpd

      envFrom:
        - secretRef:
            name: mc-secret

=================================================================


kubectl get secret 

=============================================================================
Real time scenarios : 

configmap:

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENV: "production"
  LOG_LEVEL: "info"

---------------------------------------------
( username : myuser 
  password : mypassword  ) 
  
echo -n "myuser" | base64
bXl1c2Vy

echo -n "mypassword" | base64
bXlwYXNzd29yZA==



secret:

apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  DB_USER: bXl1c2Vy
  DB_PASSWORD: bXlwYXNzd29yZA==


--------------------------------------------------
Deployment/pod : 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: myapp:1.0
        env:
          - name: SERVICE_NAME
            value: "user-service"
        envFrom:
          - configMapRef:
              name: app-config
          - secretRef:
              name: db-secret


==========================================================================




------------------------------------------------
CLUSTER UPGRADE in K8S 
=====================================================================

======================================

=============================
LABELS vs ANNOTATIONS in K8S 


---
apiVersion: v1
kind: Pod
metadata:
  name: fbpod
  labels:
    app: facebook
	env: lab
	
  annotations:
    imageregistry: "https://hub.docker.com/"
    build-url: "https://jenkins.example.com/build/142"
	prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
	
	
spec:
  containers:
  - name: nginx
    image: nginx


=============================


DATE: 24 OCT 25


ARGOCD : 

kubectl create namespace argocd

kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml -n argocd


kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'




username: admin 

Get the Initial Admin Password:
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo

==============================================================

date: 25 OCT 25

END - END CICD USING ARGOCD :

sudo yum update -y 
sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo 
sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key 
sudo yum upgrade -y 
sudo dnf install java-17-amazon-corretto -y 
sudo yum install jenkins -y 
sudo systemctl enable jenkins 
sudo systemctl start jenkins

========================
docker run 
docker build 

sudo yum install docker -y 
sudo systemctl start docker 
sudo usermod -aG docker jenkins 
sudo usermod -aG docker ec2-user 
sudo systemctl restart docker 
sudo chmod 666 /var/run/docker.sock

===========================

aws cli 

AKIASB5M6XXXXXXXXXXX
isfzCAQO+sUVaACXXXXXXXXXXXX


# -----------------------------------------
# Install kubectl
# -----------------------------------------
echo "Installing kubectl..."
curl -LO "https://dl.k8s.io/release/$(curl -sSL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
sudo mv kubectl /usr/local/bin/
kubectl version --client

# -----------------------------------------
# Install eksctl
# -----------------------------------------
echo "Installing eksctl..."
curl -sSL "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_Linux_amd64.tar.gz" -o eksctl.tar.gz
tar -xzf eksctl.tar.gz
sudo mv eksctl /usr/local/bin/
eksctl version



eksctl create cluster --name mcappcluster --nodegroup-name mcng --node-type t3.micro --nodes 8 --managed




Docker run --itd  --name sonar -p 9000:9000 sonarqube

sonartoken: 

squ_f21d96bccXXXXXXXXXXXXXXXXXXXXXXXXXXXXX




github token :

ghp_zNka9WDdsM8AXXXXXXXXXXXXXXXXXXXXXXXXXXX

==============================================================================================================


INGRESS:


DATE: 27 OCT 25

INGRESS - Advanced load balancing capabilities 

Ingress Hostbased and  path based routing 

Purchasing domain in Godaddly 
Hosting it on route-53 & creating records 

========================================================================================================

=====================================================================================

DATE: 29 OCT 25


eksctl create cluster --name b16dcluster  --nodegroup-name b16dng --managed

CREATING CLUSTER USING MANIFEST FILE: 

----------------------------------------

clustersetup.yaml 

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: b16dcluster
  region: us-east-1

managedNodeGroups:
  - name: b16d
    spot: true   ## upto 90%  discuount 
    instanceTypes: ["m5.large"]  # Use multiple types for better availability
    desiredCapacity: 2
    minSize: 2
    maxSize: 3
    labels:
      lifecycle: Ec2Spot
    tags:
      "k8s.io/cluster-autoscaler/enabled": "true"
      "k8s.io/cluster-autoscaler/b16dcluster": "owned"
    iam:
      withAddonPolicies:
        autoScaler: true
        ebs: true


================================================================================

KUBERNETS VOLUMES :

In Kubernetes, pods are temporary (ephemeral) by design. This means that any data stored inside a pod will be lost if the pod is deleted, restarted, or recreated — for example, during scaling or node failures.

To prevent data loss and keep your data available even when pods change, Kubernetes provides a feature called Volume Provisioning. This lets you attach external storage to your pods so that the data stays safe and accessible.

Two main ways to provision volumes in Kubernetes:

    Static Provisioning – You manually create a Persistent Volume (PV) ahead of time, and then link it to your pod using a Persistent Volume Claim (PVC).
    
    Dynamic Provisioning – Kubernetes automatically creates and manages storage when your pod requests it, using a predefined StorageClass.

In AWS, you can use two common storage options for persistent data:

   Amazon EBS (Elastic Block Store) – Great for single-pod, block-level storage with high performance.   
   Amazon EFS (Elastic File System) – Ideal for shared file storage that multiple pods can access at the same time.


Static Provisioning using EBS: 

In Static Provisioning, the storage volume is created manually before it is used by a pod. The Kubernetes administrator creates an EBS volume in AWS, and then defines a PersistentVolume (PV) that references it. Pods can then claim and use this storage through a PersistentVolumeClaim (PVC).

STEPS:

1. Install the Amazon EBS CSI Driver
     The EBS CSI driver is required in modern clusters. It handles attaching/detaching EBS volumes to pods.
	 
	 kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.51"
	 
2. Modify IAM ROLE & attach a policy for EBSCSI Driver permission	 
	 
2. Create EBS volume (Availability Zone (AZ) — it must match the AZ of your Kubernetes worker node ) 
3. Create a PersistentVolume (PV)
     Define a PV referencing the EBS volume using the CSI driver:
4. Create a PersistentVolumeClaim (PVC)
5. Use the PVC in a Pod
6. Verification & Testing ( Create a test file inside the pod ) 
7. Delete the pod:
8. Recreate the pod (using the same PVC) and verify the file persists:
9. This proves data persistence across pod restarts.

STATIC PROVISIOING :


kid     --> mother --> father --> wallet 
POD/APP       PVC          PV          EBS



VOLUMES , 
Persistant Volume (PV) 
Persistant Volume  Claim 

Installing EBS CSI Driver :

kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.51"

=====================================================
DYNAMIC PROV 



Dynamic Provisioning on EKS using Amazon EBS (with CSI Driver)

Dynamic provisioning allows Kubernetes to automatically create and manage EBS volumes when a pod requests storage. Unlike static provisioning, you do not manually create PVs; Kubernetes handles volume creation using a StorageClass and the EBS CSI driver.



  1.Create a StorageClass
  2. Create a PersistentVolumeClaim (PVC)
  3. Use the PVC in a Pod
  4. Apply the manifest
  5. Write a file in the pod:
  6. Delete the pod:
  7. Recreate the pod:
  
Dynamic provisioning is recommended for production environments because it reduces manual effort and prevents AZ mismatches.  

==================================


ATE: 31 OCT 25 

Kubernetes Volume Provisioning on EKS using Amazon EFS

Amazon EFS is a fully-managed, network-attached, shared file system that can be mounted by multiple pods across multiple nodes simultaneously. This makes it ideal for workloads requiring shared access, like web servers, content management systems, or big data pipelines.




 - 1.  Install the Amazon EFS CSI Driver 
 
kubectl apply -k github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-2.1

kubectl apply -k 
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-2.1" 


 - 2.  Attach AmazonEFSCSIDriverPolicy / Permission in IAM roe 
 
 - Create an Amazon EFS File System ( same VPC where your EKS cluster resides.) 
 
 - Configure security groups to allow NFS traffic (TCP 2049) from your worker nodes.
   (  Unlike EBS, EFS is network-attached and does not require AZ matching. ) 
 
 
 -Create a PersistentVolume (PV)
 -Create a PersistentVolumeClaim (PVC)
 -Use PVC in a Pod
 - LB SERVICE 
 
 ============================================================
 Dynamic Provisioning with EFS

Dynamic provisioning allows Kubernetes to create subdirectories in EFS automatically when a PVC is requested.



 - Create a StorageClass for EFS
 - Create a PVC Using the StorageClass
 - Use PVC in a Pod  , svc
 - VERIFY 




DRIVER INSTALL : 

kubectl kustomize \
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-2.X" > private-ecr-driver.yaml

kubectl kustomize \
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-2.1" > private-ecr-driver.yaml


kubectl apply -k github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-2.1




==============================================

  ==============================================
1 NOV 25 


Kubernetes StatefulSets: 

A StatefulSet is a Kubernetes controller used to manage stateful applications — workloads that require stable, unique network identifiers and persistent storage.



In Kubernetes, most workloads (like Deployments) are stateless, meaning:

   Each pod is identical.   
   It doesn’t matter which pod serves your request.   
   You can delete, recreate, or scale pods freely — no data or identity is lost.

Stateful means the application has memory or identity that must persist, such as:

    A database that stores data on disk. 
    A message broker (like Kafka, RabbitMQ) that keeps message queues. 
    Any system that requires stable storage or a unique network identity
	
	
Let’s say you’re running a 3-node mysql/MongoDB cluster:

   You can’t just start all pods at once. The primary must start first.
   Each pod must keep its own data volume (so the database doesn’t lose its data).
   Each pod must have a stable name (mongo-0, mongo-1, mongo-2) to connect to each other.

so statefulsets provides :
  
   Persistent storage
   Stable network identity
   Ordered deployment & scaling



=======================

1. Create a Headless Service: 
---- This service ensures each pod gets a stable DNS entry like:


2. Create the StatefulSet



==========================================================================================================


AUTO SCALING :
Auto Scaling means automatically adjusting resources (like servers, pods, or CPUs) based on the load on your system.

       If your app gets busy, it adds more resources (scales up).       
       If traffic is low, it removes resources (scales down).
	   Keep performance good without wasting resources.


Horizontal Auto Scaling?  (scaling out or in )  ----> by adding or removing instances /PODS 

Vertical Auto Scaling? (scaling up or down)   ---> by increasing or decreasing the resources (CPU/RAM) of the same Instance/ Pod.



HPA: (HORIZONTAL POD AUTOSCALAR) 
metric server:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-1.21+.yaml



 Run this in a separate terminal
# so that the load generation continues and you can carry on with the rest of the steps
kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"




